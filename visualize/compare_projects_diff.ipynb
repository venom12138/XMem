{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       narration                 Sequence_x  J-Mean_x  F-Mean_x  J-Mean_y  \\\n",
      "68  P22_01_164_1                 peel peach  0.000000  0.000000  0.787109   \n",
      "97   P13_01_46_1                          0  0.000000  0.000000  0.734863   \n",
      "84  P28_25_107_1                  fold wrap  0.211060  0.412109  0.830078   \n",
      "79    P28_19_1_1                chop pepper  0.010002  0.135010  0.577148   \n",
      "14   P01_14_93_1  continue dicing aubergine  0.036987  0.085999  0.595215   \n",
      "..           ...                        ...       ...       ...       ...   \n",
      "76   P28_16_26_1    continue chopping onion  0.737793  0.928223  0.618164   \n",
      "52   P11_20_59_1             chop off leeks  0.757812  0.872070  0.633789   \n",
      "3   P01_14_133_1                  cut onion  0.817871  0.932129  0.653809   \n",
      "83    P28_24_2_1        cut piece of tomato  0.767090  0.905762  0.578125   \n",
      "82   P28_24_13_1                chop tomato  0.699219  0.967773  0.501953   \n",
      "\n",
      "    F-Mean_y  Blob-Mean    J_diff    F_diff  \n",
      "68  0.915039      0.778  0.787109  0.915039  \n",
      "97  0.886230      0.626  0.734863  0.886230  \n",
      "84  0.926758      0.651  0.619141  0.515137  \n",
      "79  0.904785      0.421  0.566895  0.770020  \n",
      "14  0.780762      0.461  0.558105  0.694824  \n",
      "..       ...        ...       ...       ...  \n",
      "76  0.889160      0.606 -0.119995 -0.039001  \n",
      "52  0.793945      0.452 -0.124023 -0.078003  \n",
      "3   0.916016      0.563 -0.163940 -0.016006  \n",
      "83  0.712891      0.646 -0.188965 -0.192993  \n",
      "82  0.878906      0.502 -0.197021 -0.088989  \n",
      "\n",
      "[98 rows x 9 columns]\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imageio\n",
    "import os\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from inference.interact.interactive_utils import overlay_davis\n",
    "from PIL import Image\n",
    "\n",
    "csv_name1 = '/home/venom/.exp/XMem/1003XMem/per-sequence_results-val.csv'\n",
    "csv_name2 = '/home/venom/.exp/1019_reverse_flow/D0142_freeze=0,fuse_type=cbam,steps=1000,use_text=0,use_flow=1/eval_10000/all_per-sequence_results-val.csv'\n",
    "\n",
    "csv1 = pd.read_csv(csv_name1)\n",
    "csv2 = pd.read_csv(csv_name2)\n",
    "data_merge = pd.merge(csv1, csv2, on='narration', how='outer')\n",
    "data_merge = data_merge.drop(columns='Sequence_y')\n",
    "data_merge.fillna(0, inplace=True)\n",
    "data_merge['J_diff'] = data_merge['J-Mean_y'] - data_merge['J-Mean_x'] # csv2-csv1\n",
    "data_merge['F_diff'] = data_merge['F-Mean_y'] - data_merge['F-Mean_x'] # csv2-csv1\n",
    "data_merge = data_merge.sort_values(by='J_diff', ascending=False)\n",
    "\n",
    "data_merge[['J-Mean_x', 'J-Mean_y', 'F-Mean_x', 'F-Mean_y', 'J_diff', 'F_diff']] = data_merge[['J-Mean_x', 'J-Mean_y', 'F-Mean_x', 'F-Mean_y', 'J_diff', 'F_diff']].astype('float16')\n",
    "\n",
    "data_merge.to_csv('./tmp.csv', index=False)\n",
    "print(data_merge)\n",
    "print('----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualize images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_path = '/home/venom/projects/XMem/val_data'\n",
    "run1_dir = '/'.join(csv_name1.split('/')[:-1])\n",
    "run2_dir = '/'.join(csv_name2.split('/')[:-1])\n",
    "\n",
    "cols = 5\n",
    "rows = 3\n",
    "alpha = 0.5\n",
    "key = 'P08_10_40'\n",
    "PART = key.split('_')[0]\n",
    "video_id = '_'.join(key.split('_')[:2])\n",
    "anno_path = os.path.join(val_data_path, PART, 'anno_masks', video_id, key)\n",
    "annoted_imgs = sorted(glob(f'{anno_path}/*.png'))[1:-1]\n",
    "selected_annos = sorted(np.random.choice(annoted_imgs, size=min(len(annoted_imgs), cols), replace=False))\n",
    "cols = len(selected_annos)\n",
    "# runs draw\n",
    "# run1_dir\n",
    "for row_idx, run_path in enumerate([run2_dir]):\n",
    "    print(run_path)\n",
    "    # if not os.path.exists(run_path + '/draw'):\n",
    "    #     assert os.path.exists(run_path + '/masks.zip')\n",
    "    #     current_path = os.getcwd()\n",
    "    #     os.chdir(run_path)\n",
    "    #     os.system('unzip masks.zip')\n",
    "    #     if not os.path.exists('./draw'):\n",
    "    #         current_dir = './u'\n",
    "    #         while True:\n",
    "    #             if len(os.listdir(current_dir)) == 1:\n",
    "    #                 next_dir = os.listdir(current_dir)[0]\n",
    "    #                 current_dir = os.path.join(current_dir, next_dir)\n",
    "    #             else:\n",
    "    #                 os.system(f'mv {current_dir}/* {run_path}')\n",
    "    #                 break\n",
    "    #     os.chdir(current_path)\n",
    "    \n",
    "    # fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(150, 150))\n",
    "    run_anno_path = os.path.join(run_path, PART, video_id, key)\n",
    "    \n",
    "    # combine frames to gif\n",
    "    images = []\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=cols, figsize=(150, 150))\n",
    "    for i, frame_path in enumerate(selected_annos):\n",
    "        rgb_path = os.path.join(val_data_path, PART, 'rgb_frames', video_id, key)\n",
    "        raw_frame_path = os.path.join(rgb_path, frame_path.replace('png', 'jpg').split('/')[-1])\n",
    "        anno_path = os.path.join(run_anno_path, frame_path.split('/')[-1])\n",
    "        \n",
    "        visualization = overlay_davis(np.array(Image.open(raw_frame_path)), np.array(Image.open(anno_path)), alpha=alpha)\n",
    "        # im = plt.imread(frame_path)\n",
    "        # plt.subplot(rows,cols,row_idx*len(selected_annos)+i+1)\n",
    "        axes[i].imshow(visualization)\n",
    "        # plt.imshow(visualization)\n",
    "fig.savefig(f\"../visuals/failure_case/{key}_pred.pdf\")\n",
    "\n",
    "print(val_data_path)\n",
    "# fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(150, 150))\n",
    "anno_path = os.path.join(val_data_path, PART, 'anno_masks', video_id, key)\n",
    "rgb_path = os.path.join(val_data_path, PART, 'rgb_frames', video_id, key)\n",
    "images = []\n",
    "fig, axes = plt.subplots(nrows=1, ncols=cols, figsize=(150, 150))\n",
    "for i, frame_path in enumerate(selected_annos): \n",
    "    raw_frame_path = frame_path.replace('png', 'jpg').split('/')[-1]\n",
    "    raw_frame_path = os.path.join(rgb_path, raw_frame_path)\n",
    "    visualization = overlay_davis(np.array(Image.open(raw_frame_path)), np.array(Image.open(frame_path)), alpha=alpha)\n",
    "    # im = plt.imread(frame_path)\n",
    "    axes[i].imshow(visualization)\n",
    "    # plt.subplot(rows, cols,2*len(selected_annos)+i+1)\n",
    "    # plt.imshow(visualization)\n",
    "fig.savefig(f\"../visuals/failure_case/{key}_gt.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 10%\n",
      "('P29_05_352', {'cnt': 11, 'mean': 0.07245, 'std': 0.1316})\n",
      "('P06_10_51', {'cnt': 11, 'mean': 0.029, 'std': 0.1002})\n",
      "('P04_25_23', {'cnt': 11, 'mean': 0.01, 'std': 0.0598})\n",
      "('P08_16_63', {'cnt': 11, 'mean': -0.01628, 'std': 0.1517})\n",
      "('P11_20_74', {'cnt': 11, 'mean': -0.01845, 'std': 0.0485})\n",
      "('P15_04_26', {'cnt': 11, 'mean': -0.01973, 'std': 0.07556})\n",
      "('P04_24_55', {'cnt': 11, 'mean': -0.02336, 'std': 0.05814})\n",
      "('P07_14_42', {'cnt': 11, 'mean': -0.02563, 'std': 0.06396})\n",
      "('P29_05_355', {'cnt': 11, 'mean': -0.0269, 'std': 0.0739})\n",
      "('P01_14_133', {'cnt': 11, 'mean': -0.03293, 'std': 0.0457})\n",
      "('P08_14_29', {'cnt': 11, 'mean': -0.0338, 'std': 0.06433})\n",
      "('P11_17_44', {'cnt': 11, 'mean': -0.03427, 'std': 0.0383})\n",
      "('P06_12_11', {'cnt': 11, 'mean': -0.03445, 'std': 0.0355})\n",
      "('P03_23_82', {'cnt': 11, 'mean': -0.03708, 'std': 0.11414})\n",
      "('P29_05_394', {'cnt': 11, 'mean': -0.03983, 'std': 0.0227})\n",
      "('P02_12_321', {'cnt': 11, 'mean': -0.0402, 'std': 0.0755})\n",
      "('P07_14_22', {'cnt': 11, 'mean': -0.05228, 'std': 0.0498})\n",
      "('P22_01_165', {'cnt': 11, 'mean': -0.05664, 'std': 0.05396})\n",
      "('P04_24_56', {'cnt': 11, 'mean': -0.05847, 'std': 0.02724})\n",
      "('P01_11_24', {'cnt': 11, 'mean': -0.06744, 'std': 0.1555})\n",
      "('P22_01_169', {'cnt': 11, 'mean': -0.0931, 'std': 0.06744})\n",
      "('P22_01_114', {'cnt': 11, 'mean': -0.09515, 'std': 0.04364})\n",
      "('P11_20_37', {'cnt': 11, 'mean': -0.10046, 'std': 0.06775})\n",
      "('P29_05_350', {'cnt': 11, 'mean': -0.1007, 'std': 0.129})\n",
      "('P08_16_84', {'cnt': 11, 'mean': -0.1011, 'std': 0.1465})\n",
      "('P28_24_13', {'cnt': 11, 'mean': -0.1089, 'std': 0.1757})\n",
      "('P29_05_351', {'cnt': 11, 'mean': -0.1611, 'std': 0.2064})\n",
      "('P01_11_9', {'cnt': 11, 'mean': -0.191, 'std': 0.1128})\n",
      "('P28_24_2', {'cnt': 11, 'mean': -0.1925, 'std': 0.08246})\n",
      "('P06_10_49', {'cnt': 11, 'mean': -0.2053, 'std': 0.1819})\n",
      "('P02_12_193', {'cnt': 11, 'mean': -0.2874, 'std': 0.1989})\n",
      "('P29_05_376', {'cnt': 11, 'mean': -0.3318, 'std': 0.1904})\n",
      "-----------less than 0.5------------\n",
      "[('P28_25_72', 11), ('P28_25_107', 11), ('P28_25_50', 11), ('P28_19_1', 11), ('P08_10_40', 10), ('P29_05_376', 9), ('P18_11_62', 7), ('P11_20_85', 6), ('P29_05_351', 6), ('P01_14_91', 5), ('P28_25_30', 5), ('P28_25_89', 4), ('P29_05_352', 3), ('P02_12_193', 3), ('P29_05_350', 2), ('P01_14_343', 1), ('P08_16_63', 1), ('P28_24_13', 1), ('P04_28_8', 1), ('P06_10_49', 1), ('P18_01_40', 1)]\n"
     ]
    }
   ],
   "source": [
    "# 统计后10%的数据的频次\n",
    "import pandas as pd\n",
    "csv_names = ['/home/venom/.exp/1010_ema_ts_all_align_loss=0/D0110_teacher_warmup=100,teacher_loss_weight=0.01,use_text=0,use_flow=1/eval_2000/per-sequence_results-val.csv',\n",
    "            '/home/venom/.exp/1010_ema_ts_all_align_loss=0/D0110_teacher_warmup=100,teacher_loss_weight=0.01,use_text=0,use_flow=1/eval_10000/per-sequence_results-val.csv',\n",
    "            '/home/venom/.exp/1010_ema_ts_all_align_loss=0/D0113_teacher_warmup=100,teacher_loss_weight=0.01,use_text=1,use_flow=1/eval_2000/per-sequence_results-val.csv',\n",
    "            '/home/venom/.exp/1010_ema_ts_all_align_loss=0/D0113_teacher_warmup=100,teacher_loss_weight=0.01,use_text=1,use_flow=1/eval_10000/per-sequence_results-val.csv',\n",
    "            '/home/venom/.exp/1007_new_valdata_normal_train_CLIPL/D0082_no_align,use_text=1,use_flow=1/eval_15000/per-sequence_results-val.csv',\n",
    "            '/home/venom/.exp/1007_new_valdata_normal_train_CLIPL/D0082_no_align,use_text=1,use_flow=1/eval_11500/per-sequence_results-val.csv',\n",
    "            '/home/venom/.exp/1007_new_valdata_normal_train_CLIPL/D0085_no_align,use_text=0,use_flow=0/eval_1000/per-sequence_results-val.csv',\n",
    "            '/home/venom/.exp/1007_new_valdata_normal_train_CLIPL/D0085_no_align,use_text=0,use_flow=0/eval_15000/per-sequence_results-val.csv',\n",
    "            '/home/venom/.exp/1006_new_valdata_normal_train/D0080_no_align,use_text=0,use_flow=1/eval_4500/per-sequence_results-val.csv',\n",
    "            '/home/venom/.exp/1006_new_valdata_normal_train/D0080_no_align,use_text=0,use_flow=1/eval_15000/per-sequence_results-val.csv',\n",
    "            '/home/venom/.exp/1006_new_valdata_normal_train/D0081_dice_align,use_text=0,use_flow=1/eval_4500/per-sequence_results-val.csv',]\n",
    "\n",
    "csv_name1 = '/home/venom/.exp/XMem/1003XMem/per-sequence_results-val.csv'\n",
    "\n",
    "csv1 = pd.read_csv(csv_name1)\n",
    "# last 10% data\n",
    "counts_dict = {}\n",
    "negative_counts = {}\n",
    "for csv_name in csv_names:\n",
    "    csv2 = pd.read_csv(csv_name)\n",
    "    # df = df.sort_values(by='J-Mean', ascending=False)\n",
    "    data_merge = pd.merge(csv1, csv2, on='narration', how='outer')\n",
    "    data_merge = data_merge.drop(columns='Sequence_y')\n",
    "    data_merge.fillna(0, inplace=True)\n",
    "    data_merge['J_diff'] = data_merge['J-Mean_y'] - data_merge['J-Mean_x'] # csv2-csv1\n",
    "    data_merge['F_diff'] = data_merge['F-Mean_y'] - data_merge['F-Mean_x'] # csv2-csv1\n",
    "    data_merge = data_merge.sort_values(by='J_diff', ascending=False)\n",
    "\n",
    "    data_merge[['J-Mean_x', 'J-Mean_y', 'F-Mean_x', 'F-Mean_y', 'J_diff', 'F_diff']] = data_merge[['J-Mean_x', 'J-Mean_y', 'F-Mean_x', 'F-Mean_y', 'J_diff', 'F_diff']].astype('float16')\n",
    "    \n",
    "    for idx, item in enumerate(data_merge[-10:]['narration']):\n",
    "        item = '_'.join(item.split('_')[:-1])\n",
    "        if item in negative_counts.keys():\n",
    "            negative_counts[item]['cnt'] += 1\n",
    "            negative_counts[item]['num'].append(data_merge.iloc[idx]['J_diff'])\n",
    "        else:\n",
    "            negative_counts.update({item: {'cnt': 1, 'num': [data_merge.iloc[idx]['J_diff']]}})\n",
    "        \n",
    "        if item not in counts_dict.keys():\n",
    "            counts_dict.update({item: {'cnt': 0, 'num': []}})\n",
    "\n",
    "for csv_name in csv_names:\n",
    "    csv2 = pd.read_csv(csv_name)\n",
    "    # df = df.sort_values(by='J-Mean', ascending=False)\n",
    "    data_merge = pd.merge(csv1, csv2, on='narration', how='outer')\n",
    "    data_merge = data_merge.drop(columns='Sequence_y')\n",
    "    data_merge.fillna(0, inplace=True)\n",
    "    data_merge['J_diff'] = data_merge['J-Mean_y'] - data_merge['J-Mean_x'] # csv2-csv1\n",
    "    data_merge['F_diff'] = data_merge['F-Mean_y'] - data_merge['F-Mean_x'] # csv2-csv1\n",
    "    data_merge = data_merge.sort_values(by='J_diff', ascending=False)\n",
    "\n",
    "    data_merge[['J-Mean_x', 'J-Mean_y', 'F-Mean_x', 'F-Mean_y', 'J_diff', 'F_diff']] = data_merge[['J-Mean_x', 'J-Mean_y', 'F-Mean_x', 'F-Mean_y', 'J_diff', 'F_diff']].astype('float16')\n",
    "    \n",
    "    for key, value in negative_counts.items():\n",
    "        num = data_merge.loc[data_merge['narration']==key+'_1']['J_diff']\n",
    "        counts_dict[key]['cnt'] += 1\n",
    "        counts_dict[key]['num'].append(num)\n",
    "\n",
    "# less than 0.5\n",
    "lt_50_dict = {}\n",
    "for csv_name in csv_names:\n",
    "    df = pd.read_csv(csv_name)\n",
    "    df = df.sort_values(by='J-Mean', ascending=False)\n",
    "    for item in df[-10:]['narration']:\n",
    "        key = '_'.join(item.split('_')[:-1])\n",
    "        if key not in lt_50_dict.keys():\n",
    "            lt_50_dict.update({key: 1})\n",
    "        else:\n",
    "            lt_50_dict[key] += 1\n",
    "\n",
    "# counts_dict = sorted(counts_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "# negative_counts = sorted(negative_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "print('last 10%')\n",
    "# counts_dict = sorted(counts_dict.items(), key=lambda x: np.mean(x[1]['num']), reverse=True)\n",
    "new_counts_dict = {}\n",
    "# print(counts_dict)\n",
    "for key, value in counts_dict.items():\n",
    "    value_mean = np.mean(np.array(value['num']))\n",
    "    value_std = np.std(np.array(value['num']))\n",
    "    new_counts_dict.update({key: {'cnt':value['cnt'], 'mean': value_mean, 'std': value_std}})\n",
    "    # print(idx)\n",
    "new_counts_dict = sorted(new_counts_dict.items(), key=lambda x: x[1]['mean'], reverse=True)\n",
    "for item in new_counts_dict:\n",
    "    print(item)\n",
    "\n",
    "print('-----------less than 0.5------------')\n",
    "lt_50_dict = sorted(lt_50_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "print(lt_50_dict)\n",
    "# print(new_counts_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### deprecated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_path = '/home/venom/projects/XMem/val_data'\n",
    "run1_dir = '/home/venom/.exp/1003XMem'\n",
    "run2_dir = '/home/venom/.exp/1010_ema_ts_all_align_loss=0/D0110_teacher_warmup=100,teacher_loss_weight=0.01,use_text=0,use_flow=1/eval_3000'\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "df = df.sort_values(by='J_diff', ascending=False)\n",
    "df_positive = df[df['J_diff'] > 0.1]\n",
    "df_negative = df[df['J_diff'] < -0.1]\n",
    "print('--positive--')\n",
    "for key in df_positive['narration']:\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=5, figsize=(150, 150))\n",
    "    key = '_'.join(key.split('_')[:-1])\n",
    "    PART = key.split('_')[0]\n",
    "    video_id = '_'.join(key.split('_')[:2])\n",
    "    rgb_path = os.path.join(val_data_path, PART, 'rgb_frames', video_id, key)\n",
    "    selected_frames = np.random.choice(sorted(glob(f'{rgb_path}/*.jpg')),size=5,replace=False)\n",
    "    # combine frames to gif\n",
    "    images = []\n",
    "    print(key)\n",
    "    for i, frame_path in enumerate(selected_frames):\n",
    "        im = plt.imread(frame_path)\n",
    "        axes[i].imshow(im)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--negative--')\n",
    "for key in df_negative['narration']:\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=5, figsize=(150, 150))\n",
    "    key = '_'.join(key.split('_')[:-1])\n",
    "    PART = key.split('_')[0]\n",
    "    video_id = '_'.join(key.split('_')[:2])\n",
    "    rgb_path = os.path.join(val_data_path, PART, 'rgb_frames', video_id, key)\n",
    "    selected_frames = np.random.choice(sorted(glob(f'{rgb_path}/*.jpg')),size=5,replace=False)\n",
    "    # combine frames to gif\n",
    "    images = []\n",
    "    print(key)\n",
    "    for i, frame_path in enumerate(selected_frames):\n",
    "        im = plt.imread(frame_path)\n",
    "        axes[i].imshow(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('Pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "cde3319b31b61703db873ffb58ba859eb15e566f9b9975ca5ed85f3392c8260c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
